\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amsfonts,amssymb,amsthm,mathtools}
\usepackage{parskip}
\usepackage{color}
\usepackage{booktabs}

\newtheorem{exercise}{Exercise}
\newtheorem{answer}{Answer}

\newcommand{\dd}[2][]{\frac{\partial #1}{\partial #2}}
\newcommand{\yh}{\hat{y}}

\newcommand{\bracket}[3]{\left#1 #3 \right#2}
\newcommand{\sqb}{\bracket{[}{]}}
\renewcommand{\b}{\bracket{(}{)}}
\newcommand{\abs}{\bracket{\lvert}{\rvert}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\f}{\mathbf{f}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\W}{\mathbf{W}}
\newcommand{\I}{\mathbf{I}}
\renewcommand{\P}{\operatorname{P}\b}

\newcommand{\w}{\mathbf{w}}
\newcommand{\wo}{\w^*}

\renewcommand{\L}{\mathcal{L}}
\newcommand{\E}{\operatorname{E}\sqb}
\newcommand{\Var}{\operatorname{Var}\sqb}

\newcommand{\logits}{\ell}
\newcommand{\vlogits}{\boldsymbol{\logits}}
\newcommand{\softmax}{\operatorname{softmax}\b}

\title{SEMT20003, Part 3: Linear classification}
\author{Laurence Aitchison}
\date{}

\begin{document}

\maketitle

\section{An loss function for classification error}

Doing regression by gradient descent is pretty straightforward, because all the distances between the real data, $y_i$ and targets, $\yh(\x_i)$ are continuous and differentiable (almost everywhere).
We can therefore define the loss as the sum-of-squared-errors.
However, things become much harder when it comes to classification.
In classification, we take inputs (here, $x_0$ and $x_1$) and try to determine which of a fixed number of discrete classes the datapoint belongs to:
\begin{center}
  \includegraphics{clust_example.pdf}
\end{center}
Looking back at regression, the starting point was to define a prediction-function which takes $x_0$ and $x_1$ as inputs, gives predictions as outputs, and has parameters that you can tweak.
For instance, if we have two classes (class 0 and class 1) we could use,
\begin{align}
  \yh_\w(\x) &= \Theta(\x \w)
\end{align}
where $\Theta(a)$ is the Heaviside step function, which returns $0$ when the input argument is negative ($a <0$), and returns $1$ otherwise.
\begin{center}
  \includegraphics{heaviside.pdf}
\end{center}
As $\yh_\w(\x)$ returns $0$ or $1$, and is parameterised in terms of a weight-vector, $\w$, we can treat the output from $\yh_\w(\x)$ as a prediction of the class (class $0$ or class $1$). We can then optimize the weights in order to give better predictions.

Key problem: what loss function to use?
The obvious choice is classification error:
\begin{align}
  \label{eq:class_error}
  \L(\w) &= \text{number of wrong classifications}\\
  \L(\w) &= \sum_{i=1}^N \text{different}(y_i, \yh_\w(\x_i))
  \intertext{where,}
  \text{different}(y, \hat{y}) &= 
  \begin{cases} 
    0 &\text{if } y = \hat{y}\\
    1 &\text{otherwise}
  \end{cases}
\end{align}
The problem is that the number of wrong classifications is always an integer (0, 1, 2 etc.)
\begin{center}
  \includegraphics{non_diff_loss.pdf}
\end{center}
This function isn't continuous. In fact, the gradient is flat (almost) everywhere! So we can't do gradient descent!

Instead, we need an alternative loss, which is continuous and differentiable (almost everywhere).
How do we get such a loss?
We're going to set up classification as maximum-likelihood in a probabilistic model.  To introduce the basic idea, we first need to take a detour through maximum-likelihood for coin flipping...

\section{Maximum likelihood for coin flipping}

%That's all fine at a very high-level.
%But to ground everything, we need to start by working with a concrete distribution, $\P{x| \theta}$.
%Perhaps the simplest concrete distribution arises from tossing a biased coin.
%We're therefore going to use maximum likelihood to fit the probability, $p$.
%Once we have $p$, we can also sample new coin tosses that look like those in the data.

When tossing a biased coin, our random variable, $x$, can take on values of $1$ or $0$.
In coin flipping, $x=1$ corresponds to heads and $x=0$ corresponds to tails.
In classification (which we'll get on to), $x=1$ corresponds to class 1, while $x=0$ corresponds to class $0$ (logically enough).
Because this is a biased coin, there is a parameter, $p$, which controls the probability of heads vs tails (or class 0 vs class 1).
Specifically,
\begin{subequations}
\begin{align}
  \label{eq:bern}
  \P{x=1| p} &= p\\
  \P{x=0| p} &= (1-p).
\end{align}
\end{subequations}
The probability of heads \textit{or} tails adds to $1$,
\begin{align}
  1 = \P{x=1| p} + \P{x=0| p} &= p + (1-p).
\end{align}
For the maths, it'll be super-useful to have a form for the probability which takes $x$ as an input (rather than just having two different forms for $x=1$ and $x=0$, as in Eq.~\ref{eq:bern}).
We can write this as,
\begin{align}
  \label{eq:coin}
  \P{x| p} &= p^x (1-p)^{1-x}
  \intertext{Note that there isn't really a ``derivation'' for this.  It just happens to be an expression gives the right answer when we substitute $x=1$ or $x=0$,}
  \P{x=1| p} &= p^1 (1-p)^0 = p \times 1 = p,\\
  \P{x=0| p} &= p^0 (1-p)^1 = 1 \times (1-p) = (1-p).
\end{align}

Now, lets consider a classic problem: we have a dataset of tosses from a biased coin, $x_1,\dotsc,x_N$, and our goal is to estimate the probability, $p$, that generated these coin tosses.
To do that, we find the $p$ that makes the observed data most probable (i.e.\ maximum likelihood),
\begin{align}
  \L(p) &= \log \P{x_1,\dotsc,x_N| p}
\end{align}
Specifically, we maximize the maximize the \textit{log} probability.
As the logarithm is a monotonically increasing function, this gives the right answer.
Specifically, the value of $p$ at which $\P{x_1,\dotsc,x_N| p}$ is maximized is the same as the value of $p$ at which $\log \P{x_1,\dotsc,x_N| p}$ is maximized,
\begin{align}
  p^* &= \operatorname*{argmax}_p \P{x_1,\dotsc,x_N| p} = \operatorname*{argmax}_p \log \P{x_1,\dotsc,x_N| p}.
\end{align}
Moreover, using logarithms is very useful for two reasons discussed in the Prerequisites:
\begin{itemize}
  \item Logarithms turn products into sums, and its much easier to differentiate sums than products.
  \item Raw probabilities can be very large or very small once we take products over a large number of datapoints.  That can cause numerical issues (i.e. over/underflow).  
\end{itemize}
Now, we assume that coin tosses are independent, so $\P{x_1,\dotsc,x_N| p} =\prod_{i=1}^N \P{x_i| p}$, and 
\begin{align}
  \L(p) &= \log \prod_{i=1}^N \P{x_i| p} \\
  \intertext{The log turns the product into a sum,}
  \L(p) &= \sum_{i=1}^N \log \P{x_i| p}\\
  \intertext{Substituting the log-probability for tossing a biased coin (Eq.~\ref{eq:coin}),}
  \L(p) &= \sum_{i=1}^N \log \b{p^{x_i} (1-p)^{1-x_i}}\\
  \intertext{The log again turns the product into a sum,}
  \L(p) &= \sum_{i=1}^N \b{\log\b{p^{x_i}} + \log\b{(1-p)^{1-x_i}}}
  \intertext{And the log turns powers into products,}
  \label{eq:coin_flip_prob}
  \L(p) &= \sum_{i=1}^N \b{x_i\log p + (1-x_i) \log(1-p)}
  \intertext{Applying the sum to each term separately, and noting that $p$ is independent of $i$,}
  \label{eq:coin_flip_ll}
  \L(p) &= \underbrace{\b{\sum_{i=1}^N x_i}}_{\text{a constant number}} \log p + \underbrace{\b{N - \sum_{i=1}^N x_i}}_{\text{another constant number}}\log(1-p)
  \intertext{So we can treat this as,}
  \L(p) &= a \log p + b \log(1-p)
\end{align}
where,
\begin{align}
  a &= \sum_{i=1}^N x_i&
  b &= N - \sum_{i=1}^N x_i
\end{align}
Notice that we've extracted the sums over data into constant multipliers, $a$ and $b$.
This has simplified the loss, $\L(p)$, dramatically.
So finding the maximum-likelihood value for $p$ is now a tractable, univariate calculus problem!

Moreover, notice that despite the data, $x_i$, being discrete, the objective is a continuous and differentiable function of the parameter, $p$. 
As such, we can find the most likely probability, $p^*$, by doing gradient descent!
If we did that, we'd end up with the sensible answer (see Exercises),
\begin{align}
  p^* &= \tfrac{1}{N} \sum_{i=1}^N x_i.
\end{align}


\section{A smooth, maximum-likelihood objective for binary classification}
\label{sec:binary_obj}
In the previous section, we saw that we can get a continuous, differentiable loss for discrete data by setting the problem up as maximum-likelihood.
So how do we apply the same idea to get a differentiable loss for binary classification?
The basic idea is that we first do the usual thing of taking the dot-product of the input vector with some weights,
\begin{align}
  \logits = f_\w(\x_i) &= \x_i \w.
\end{align}
The problem is that the result, $\logits$, could be from $-\infty$ to $\infty$, while we want a probability, between $0$ and $1$.
If the input features, $\x_i$ are bounded (e.g.\ they could be bounded between $0$ and $1$), then we could find bounds on $\w$ that ensure that the result of this dot-product is always between $0$ and $1$.
But that sounds hard.
And it won't work in more complex architectures like neural networks, where we can't find the required constraints on the parameters.

As such, we need to do something else. 
In particular, we don't take take $\logits$ to be the probability; instead, we take $\logits$ to be the ``logits'', which could be anything from $-\infty$ to $\infty$.
We can transform from the logits to the probability by applying the ``logistic sigmoid'' function (or sigmoid\footnote{From Wikipedia: a sigmoid function is a mathematical function having a characteristic ``S''-shaped curve or sigmoid curve, so the logistic sigmoid is a specific function with an ``S''-shaped curve.} for short), denoted $p = \sigma(\logits)$.
The sigmoid function takes any $\logits$ (from $-\infty$ to $\infty$) and returns a number from $0$ to $1$:
\begin{align}
  \sigma(\logits) &= \frac{1}{1+e^{-\logits}}
\end{align}
Graphically, the sigmoid looks like,
\begin{center}
  \includegraphics{sigmoid.pdf}
\end{center}
Now, we can use the logistic sigmoid to give the probability for class $1$ for input $\x$,
\begin{align}
  \label{eq:sigmoid_prob}
  p_\w(\x) &= \sigma(f_\w(\x))\\
  \P{y=1| \x_i} &= p_\w(\x_i).
\end{align}

\section{A smooth, maximum-likelihood objective for multi-class classification}
\label{sec:multi_obj}
In the previous section, we were looking at classification problems where there's two possible classes.
But almost always, there's multiple classes.
For instance, we might want to classify handwritten digits as 0--9, in which case, there's ten classes.
\begin{center}
\includegraphics[width=0.7\textwidth]{mnist}
\end{center}
Again, to train a model for multi-class classification, we need an objective.
Classification error (Eq.~\ref{eq:class_error}) also makes sense in the multi-class setting, but again, can't be used as an objective as it is non-differentiable.
As in the binary classification setting, we need an objective that is a continuous, differentiable function of the parameters (weights).
To get such an objective, we're again going to consider a probability distribution over classes.
In multi-class classification, the probability distribution is over the integers $0$--$9$ (or in general, over the integers $0$--$C$, where $C$ is the number of classes), instead of just over $\{0, 1\}$, as in binary classification in the previous section.

The first problem is that previously, we just had one scalar logits for each datapoint, $\logits = f_\w(\x_i) = \x_i \w$, and we can't turn just a single scalar into a probability distribution over multiple classes.
Instead, in multi-class classification, we use a whole \textit{vector} of logits, $\vlogits$, for each datapoint.
This vector has length $C$, so there's one logits for each class,
\begin{align}
  \underbrace{\vlogits}_{1 \times C} = \f_{\W}(\x) = \underbrace{\x}_{1 \times D} \underbrace{\W}_{D \times C}.
\end{align}
Note that we get a vector of logits by replacing the usual $D \times 1$ weight vector, $\w$, with a $D\times C$ weight matrix, $\W$.

Now we have a vector of $C$ logits, $\vlogits$.
But we are again, left with the problem of how to set the probability for each class, given that the individual logits values can be anywhere from $-\infty$ to $\infty$.
We use the so-called ``softmax'' function,
\begin{align}
  p_c(\x) &= \softmax{\vlogits = \f_{\W}(\x)} = \frac{e^{\logits_c}}{\sum_{c'} e^{\logits_{c'}}}
\end{align}
where $c$ is the class label.
Remember that the probabilities must all be non-negative and sum to $1$.
Non-negativity is straightforward, as the logits are real, so all the exponentials are non-negative.
Further, we can check that the resulting probabilities sum to $1$,
\begin{align}
  \sum_{c} p_c(\x) &= \frac{\sum_c e^{\logits_c}}{\sum_{c'} e^{\logits_{c'}}} = 1
\end{align}
as the numerator and denominator are the same.

\section{Maximum likelihood vs cross-entropy loss}

While the maximum likelihood perspective is the right way to motivate the binary and multi-class classification objectives, deep learning types don't use this viewpoint.
Instead, they typically say that in classification, we use a ``cross entropy loss''.
Mathematically, this is just the negative log-probability, so it is equivalent to the maximum likelihood framework above (maximizing the log-probability is equivalent to minimizing the negative log-probability),
\begin{align}
  \L &= - \sum_i \log p_{y_i}(\x_i).
\end{align}
Remember, this is a sum over datapoints, and $p_{y_i}$ picks out the predicted probability of the actual target class.

Why do they call it a ``cross-entropy''?
If we look at the general definition of cross-entropy, it is a function of \textit{two} probability distributions,
\begin{align}
  H(q, p) &= - \sum_c q_c \log p_c
\end{align}
where the sum is now over image classes, rather than datapoints.
If we make a couple of choices, this becomes equal to the loss for a single datapoint.
Specifically, we take $q_c$ to be a distribution which puts all its mass at the true target class, and we take $p_c$ to be our predicted probabilties,
\begin{align}
  q_c &= \delta_{c, y_i}\\
  p_c &= p_{c}(\x_i).
  \intertext{Then,}
  H(q, p) &= - \sum_c q_c \log p_c \\
  H(q, p) &= - \sum_c \delta_{c, y_i} \log p_{c}(\x_i).
  \intertext{The Kronecker-delta picks out the element of the sum for which $c=y_i$,}
  H(q, p) &= - \log p_{y_i}(\x_i).
\end{align}
And this is exactly our loss for the $i$th datapoint.
We can simplify this loss, and rename it to a form that is more usual in deep learning.
In the multiclass setting:
\begin{align}
  \text{cross entropy}(\vlogits, y) &= -\logits_y + \log \sum_{c} \exp(\logits_c).
\end{align}
where $y$ is the target class label, and $\vlogits$ is the vector of logits that would usually come from the neural network.
In the binary classification setting,
\begin{align}
  \text{cross entropy}(\logits, y) &= \begin{cases} 
    \log (1+e^{-\logits}) & \text{if } y=1\\
    \log (1+e^{\logits}) & \text{if } y=0
  \end{cases}
\end{align}

\section{Gradient descent for binary classification}
Now, we have discussed the loss function for binary and multiclass classification, we need to think about how to actually use this objective to optimize the weights.
We'll focus on the binary setting (to keep things slightly simpler) and use gradient descent (i.e. compute the gradient of the loss wrt the weights).
But first-things-first, it turns out to be useful to rearrange the objective itself into a simpler form.
Remember that the objective is the log-probability of class-labels under our model,
\begin{align}
  \L(\w) &= - \sum_{i=1}^N \log \P{y_i| \w, \x_i}\\
  \intertext{Using the coin-flipping probability from Eq.~\eqref{eq:coin}}
  \L(\w) &= - \sum_{i=1}^N \log (p_{\w}(\x_i)^{y_i} (1-p_{\w}(\x_i))^{1-y_i})\\
  \intertext{The log turns products into sums,}
  \L(\w) &= - \sum_{i=1}^N \sqb{\log (p_{\w}(\x_i)^{y_i}) + \log((1-p_{\w}(\x_i))^{1-y_i})}.\\
  \intertext{Next, the log turns powers into products, (mirroring the coin flipping result in Eq.~\ref{eq:coin_flip_prob})}
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log (p_{\w}(\x_i)) + (1-y_i)\log((1-p_{\w}(\x_i)))}.\\
  \intertext{Substituting the value of $p_{\w}(\x_i)$ (Eq.~\ref{eq:sigmoid_prob}), (keeping the sigmoid in the first term, but using the explicit form for the sigmoid in the second term),}
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\b{1 - \frac{1}{1+e^{-\w \x_i}}}}
  \intertext{Using, $1 = (1+e^{-\w \x_i}) /(1+e^{-\w \x_i})$,}
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\b{\frac{1+e^{-\w \x_i}}{1+e^{-\w \x_i}} - \frac{1}{1+e^{-\w \x}}}},\\
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\frac{1 +e^{-\w \x_i} - 1}{1+e^{-\w \x_i}}},\\
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\frac{e^{-\w \x_i}}{1+e^{-\w \x_i}}}.
  \intertext{Multiplying the numerator and denominator of the fraction by $e^{\w \x_i}$,}
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\frac{(e^{-\w \x_i}) e^{\w \x_i}}{(1+e^{-\w \x_i}) e^{\w \x_i}}},\\
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log\frac{1}{1 + e^{\w \x_i}}}
  \intertext{Finally, notice that the final term is another sigmoid,}
  \L(\w) &= - \sum_{i=1}^N \sqb{y_i\log \sigma(\w \x_i) + (1-y_i)\log \sigma(-\w \x_i)}
\end{align}

Now, we want to differentiate this objective wrt a weight.
This is a bigger, more complicated and scarier function than any we've seen before.
But things are going to be okay if we write out a stepwise process for computing the function, and apply the chain rule everywhere!
Specifically, we're going to compute the chain-rule using the following sequence of computations.
We're first going to compute the product of features and weights,
\begin{align}
  a^i &= \sum_\lambda w_\lambda X_{\lambda i}\\
  \intertext{Then we're going to apply the sigmoid to $a^i$ (for the first term) and $-a^i$ (for the second term),}
  s^i_0 &= \sigma(-a^i) &
  s^i_1 &= \sigma(a^i),\\
  \intertext{Then we're going to apply the log to both of the resulting probabilities}
  l^i_0 &= \log s^i_0 & 
  l^i_1 &= \log s^i_1,
  \intertext{Then we're going to compute the objective,}
  \L(\w) &= -\sum_{i=1}^N \sqb{y_i l^i_1 + (1-y_i) l^i_0}.
\end{align}
The gradient is therefore,
\begin{align}
  \dd[\L(\w)]{w_\nu} &= -\sum_{i=1}^N \sqb{y_i \dd[l^i_1]{w_\nu} + (1-y_i) \dd[l^i_0]{w_\nu}}.
  \intertext{Applying the chain rule,}
  \dd[\L(\w)]{w_\nu} &= -\sum_{i=1}^N \sqb{y_i \dd[l^i_1]{s^i_1} \dd[s^i_1]{a^i_1} \dd[a^i]{w_\nu} + (1-y_i) \dd[l^i_0]{s^i_0} \dd[s^i_0]{a^i} \dd[a^i_0]{w_\nu}}.
\end{align}
Now, we need to compute the individual derivatives in the chain rule.
To start, we look at the linear combination of weights and features,
\begin{align}
  \dd[a^i]{w_\nu} &= \dd{w_\nu}\sqb{\sum_\lambda w_\lambda X_{\lambda i}} = \sum_\lambda \delta_{\nu \lambda} X_{\lambda i} = X_{\nu i}\\
  \intertext{(see the last exercise of ``Mathematical Prerequisites'' for details).  For the sigmoid,}
  \dd[s^i_0]{a^i} &= \dd[\sigma(-a^i_0)]{a^i_0} = -\sigma(a^i) \sigma(-a^i)\\
  \dd[s^i_1]{a^i} &= \dd[\sigma(a^i_1)]{a^i_1} = \sigma(a^i) \sigma(-a^i)\\
  \intertext{(You're going to work out the form for this derivative in the exercises.)  And finally the log,}
  \dd[l^i_0]{s^i_0} &= \dd[\log s^i_0]{s^i_0} = \frac{1}{s^i_0} = \frac{1}{\sigma(-a^i)}\\
  \dd[l^i_1]{s^i_1} &= \dd[\log s^i_1]{s^i_1} = \frac{1}{s^i_0} = \frac{1}{\sigma(a^i)}
\end{align}
Putting everything back together, we get something reasonably simple,
\begin{align}
  \dd[\L(\w)]{w_\nu} &= - \sum_{i=1}^N \sqb{y_i \dd[l^i_1]{s^i_1} \dd[s^i_1]{a^i_1} \dd[a^i_1]{w_\nu} + (1-y_i) \dd[l^i_0]{s^i_0} \dd[s^i_0]{a^i_0} \dd[a^i]{w_\nu}}\\
  \dd[\L(\w)]{w_\nu} &= - \sum_{i=1}^N X_{i \nu} \sqb{y_i \frac{\sigma(a^i) \sigma(- a^i)}{\sigma(a^i)} + \frac{-\sigma(a^i) \sigma(- a^i)}{\sigma(-a^i)} (1-y_i) }\\
  &= - \sum_{i=1}^N X_{i \nu} \sqb{y_i \sigma(-a^i) - (1-y_i) \sigma(a^i)}
\end{align}
While we \textit{could} compute these derivatives, it was pretty darn tedious.
It would be great if we could get the computer to do it for us!
Next week, we're going to look at neural networks themselves, and see that PyTorch somehow magically computes the gradients for us.
The week after, we're going to see precisely \textit{how} PyTorch does this magic!


\section{Exercises}

\begin{exercise}
  Find the maximum-likelihood probability for a biased coin, given a dataset of $N$ tosses, $x_i$, with $x_i=0$ representing tails and $x_i=1$ representing heads.
  Start from the expression for the log-likelihood in Eq.~\ref{eq:coin_flip_ll}, and solve for the value of $p$ at which the gradient of the log-likelihood is zero.
\end{exercise}

\begin{exercise}
  Show that the gradient of a sigmoid is
  \begin{align}
    \dd[\sigma(a)]{a} = \sigma(a) \sigma(-a)
  \end{align}
\end{exercise}

\section{Answers}


\begin{answer}
Now, we find the maximum likelihood value of $p$ by solving for where the gradient is zero,
\begin{align}
  0 &= \dd[\L(p)]{p} \\
  0 &= \b{\sum_{i=1}^N x_i} \dd[\log p]{p} + \b{N - \sum_{i=1}^N x_i} \dd[\log(1-p)]{p}
\end{align}
It is a standard result that,
\begin{align}
  \dd[\log p]{p} &= \frac{1}{p}.
  \intertext{But to compute the other derivative, we need to apply the chain rule.  Specifically, we use,}
  u &= (1-p)\\
  y &= \log(1-p) = \log u.
  \intertext{Thus,}
  \dd[y]{p} &= \dd[y]{u} \dd[u]{p}\\
   &= \dd[\log u]{u} \dd{p}[1-p]\\
   &= \frac{1}{u}\times(-1)\\
   &= - \frac{1}{1-p}
\end{align}
Substituting in the derivatives,
\begin{align}
  0 &= \b{\sum_{i=1}^N x_i}\frac{1}{p} - \b{N - \sum_{i=1}^N x_i} \frac{1}{1-p}
  \intertext{To get the $p$'s out of the denominators, we multiply both sides by $p (1-p)$,}
  0 &= \b{\sum_{i=1}^N x_i}\frac{p (1-p)}{p} - \b{N - \sum_{i=1}^N x_i} \frac{p (1-p)}{1-p}\\
  \intertext{And cancel terms,}
  0 &= \b{\sum_{i=1}^N x_i}(1-p) - \b{N - \sum_{i=1}^N x_i} p.
  \intertext{Now, we separate out all the terms,}
  0 &= \b{\sum_{i=1}^N x_i} - \b{\sum_{i=1}^N x_i}p - N p + \b{\sum_{i=1}^N x_i} p.
  \intertext{The plus and minus $\b{\sum_{i=1}^N x_i}p$ terms cancel,}
  0 &= \b{\sum_{i=1}^N x_i} - N p.
  \intertext{Now, we add $N p$ to both sides,}
  Np &= \sum_{i=1}^N x_i
  \intertext{And divide both sides by $N$,}
  p &= \tfrac{1}{N} \sum_{i=1}^N x_i.
\end{align}
\end{answer}

\begin{answer}
  Find the gradient of a sigmoid,
  \begin{align}
    \dd[\sigma(a)]{a} &= \dd{a} \frac{1}{1+e^{-a}}
    \intertext{Apply the chain rule, with $u=1+e^{-a}$,}
    \sigma(a) &= u^{-1}\\
    \dd[\sigma(a)]{a} &= \dd[u^{-1}]{u} \dd[u]{a}
    \intertext{The individual derivatives are,}
    \dd[u^{-1}]{u} &= -u^{-2}\\
    \dd[u]{a} &= \dd{a}\sqb{1+e^{-a}} = - e^{-a}
    \intertext{Putting everything back together,}
    \dd[\sigma(a)]{a} &= \dd[u^{-1}]{u} \dd[u]{a} = \frac{e^{-a}}{u^2} = \frac{e^{-a}}{(1+e^{-a})^2}
    \intertext{Split up the denominator,}
    \dd[\sigma(a)]{a} &= \frac{1}{1+e^{-a}} \frac{e^{-a}}{1+e^{-a}}\\
    \intertext{The $1/(1+e^{-a})$ term is a sigmoid. For the other term, we multiply the numerator and denominator by $e^{a}$,}
    \dd[\sigma(a)]{a} &= \sigma(a) \frac{1}{(1+e^a)}\\
    \intertext{Now, $1/(1+e^a)$ term is also a sigmoid, just with the argument negated,}
    \dd[\sigma(a)]{a} &= \sigma(a) \sigma(-a).
  \end{align}
\end{answer}

\end{document}

